{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR parser\n",
    "This notebook contains both theory and implementation of LR(0) parser according to the\n",
    "[Dragon Book](https://en.wikipedia.org/wiki/Compilers:_Principles,_Techniques,_and_Tools).\n",
    "\n",
    "LR parser is a bottom-up parser that can parse context-free languages in linear time.\n",
    "It reads input tokens, concatenates them into AST nodes in hope that\n",
    "at the end the whole input will collapse into one big AST node, which will be the AST itself.\n",
    "If you read this notebook in hopes of understanding LR parser,\n",
    "please make sure you have already understood what is\n",
    "[CFG](https://en.wikipedia.org/wiki/Context-free_grammar) and\n",
    "[AST](https://en.wikipedia.org/wiki/Abstract_syntax_tree),\n",
    "since I wan't cover them here.\n",
    "\n",
    "\n",
    "### LR(0) parser\n",
    "LR(0) parser is the simplest one. It's also sometimes called SLR.\n",
    "* S - Simple. But I can't call it simple: it's much more complex than PEG or LL.\n",
    "* L - Left-to-right: the parser reads an input from left to right without peeking at the end of the input.\n",
    "* R - Rightmost derivation in reverse: the parser builds tree by operating on the right end of list of nodes.\n",
    "\n",
    "I don't yet understand what zero in parenthes means, I will find it out when I will be writing LR(1) parser.\n",
    "\n",
    "This notebook contains the theory and the code of LR(0) parser\n",
    "splitted into bunch of sections. Every section consists of **header**, description and `the code itself`.\n",
    "But before that I will tell you the core idea of LR parser:\n",
    "> To build LR parser one should take finite automaton of LL parser with conflicts and\n",
    "resolve them by transformating this this nondeterministic finite automaton into deterministic one.\n",
    "Obtained finite automaton is the LR parser.\n",
    "\n",
    "I don't expect anyone to understand what I just wrote,\n",
    "but for me that description of the parser have divided my life into before and after:\n",
    "before I understood the thing and after. So I had to include it in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example context-free grammar\n",
    "Before doing any kind of experiments with grammars, we need a lab rat.\n",
    "For that purpose I have copy-pasted rules of a CFG grammar from [wikipedia](https://en.wikipedia.org/wiki/Context-free_grammar#Well-formed_parentheses).\n",
    "But I don't force you to use it:\n",
    "you can change the variable `grammar_source` to your own lab rat and see if the experiment give the same outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_source = \"\"\"\n",
    "    S → S S\n",
    "    S → ( S )\n",
    "    S → ( )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These rules correspond to context-free grammar,\n",
    "that desribes context-free language that contains these sentences:\n",
    "    \n",
    "    (), (()), ()(), (()()), ((())()), (()(()(()))), ()()()()()(), (())((()))(((())))\n",
    "\n",
    "In other words, this is the grammar of well-formed parentheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing grammar rules\n",
    "Plain text rules are cool, but we need to represent them with some kind of data structure.\n",
    "I will NamedTuple for that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S → S S\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "Rule = namedtuple(\"Rule\", [\"head\", \"body\"])\n",
    "Rule.__str__ = lambda rule: rule[0] + \" → \" + \" \".join(rule[1])\n",
    "print(Rule(\"S\", (\"S\", \"S\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser of rules\n",
    "Everyone knows that to write a parser you have to write a parser.\n",
    "So here is the code of a parser of grammar rules.\n",
    "We need this parser to translate our lab rat into list of rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rule(head='S', body=('S', 'S')),\n",
       " Rule(head='S', body=('(', 'S', ')')),\n",
       " Rule(head='S', body=('(', ')'))]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_rules(source):\n",
    "    for rule in source.strip().split(\"\\n\"):\n",
    "        head, body = rule.strip().split(\" → \")\n",
    "        yield Rule(head, tuple(body.split(\" \")))\n",
    "list(parse_rules(grammar_source))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive variables, terminals, start symbol from rules\n",
    "Mathematically speaking we [should](https://en.wikipedia.org/wiki/Context-free_grammar#Formal_definitions) specify variables, terminals, rules and start symbol in order to call it a grammar,\n",
    "but we(programmer) are too lazy for this.\n",
    "Why write all this when you can simply extract all the information you need solely from the rules of grammar.\n",
    "So instead of specifying all these things I wrote a function `derive_symbols()`\n",
    "to derive terminals and variables from grammar rules.\n",
    "The idea of derivation is based on the fact, that a variable can be rule head,\n",
    "while a terminal may occure only in the body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables={'S'}\n",
      "terminals={')', '('}\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "def derive_symbols(rules):\n",
    "    variables = {variable for variable, body in rules}\n",
    "    terminals = {t for _, body in rules for t in body if t not in variables}\n",
    "    return variables, terminals\n",
    "variables, terminals = derive_symbols(rules)\n",
    "print(f\"{variables=}\\n{terminals=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will assume the head of the first rule to be the start symbol.\n",
    "this assumption is based solely on the fact that it is true for **my** lab rat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start symbol: E\n",
      "variables: B, E\n",
      "terminals: '*', '+', '0', '1'\n",
      "rules:\tB → 0\n",
      "\tB → 1\n",
      "\tE → B\n",
      "\tE → E * B\n",
      "\tE → E + B\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import dataclasses\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class Grammar:\n",
    "    variables: set[str]\n",
    "    terminals: set[str]\n",
    "    rules: list[(str, tuple[str])]\n",
    "    start: str\n",
    "        \n",
    "    def __str__(self):\n",
    "        s = \"start symbol: \" + self.start + \"\\n\"\n",
    "        s += \"variables: \" + \", \".join(sorted(map(str, self.variables))) + \"\\n\"\n",
    "        s += \"terminals: \" + \", \".join(sorted(map(repr, self.terminals))) + \"\\n\"\n",
    "        rules = [f\"{var} → {' '.join(body)}\" for var, body in self.rules]\n",
    "        return s + \"rules:\\t\" + \"\\n\\t\".join(sorted(rules)) + \"\\n\"\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return id(self)\n",
    "grammar = Grammar(variables, terminals, rules, start)\n",
    "print(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR(0) Items\n",
    "LR(0) items are just rules with dot in body, e.g. `E → E •+ B`, `E → •B`, `B → 0•`.\n",
    "Items indicate that the parser has recognized a string correspondig to the part of rule before the dot,\n",
    "e.g. `E → E * •B` means that the parser has recognize `E` and `*` on the input and now expects to read `B`.\n",
    "\n",
    "I decided to make a class `Item`. It's absolutely not necessary and\n",
    "it's only purpose is to nicely print the item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item(variable='V', body=('A', 'B', 'C', 'D'), dot_position=2)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dataclasses\n",
    "@dataclasses.dataclass(frozen=True, order=True)\n",
    "class Item:\n",
    "    variable: str\n",
    "    body: tuple[str]\n",
    "    dot_position: int\n",
    "    \n",
    "    def __str__(self):\n",
    "        body = list(self.body)\n",
    "        if self.dot_position == len(body):\n",
    "            return f\"{self.variable} → {' '.join(self.body)}\" + \"•\"\n",
    "        body[self.dot_position] = \"•\" + body[self.dot_position]\n",
    "        return f\"{self.variable} → {' '.join(body)}\"         \n",
    "    \n",
    "    @staticmethod\n",
    "    def from_str(s):\n",
    "        variable, body = s.strip().split(\" → \")\n",
    "        body = body.split(\" \")\n",
    "        for dot_position, symbol in enumerate(body + [\"•\"]):\n",
    "            if symbol.startswith(\"•\"):\n",
    "                break\n",
    "        body = tuple(symbol.strip(\"•\") for symbol in body)\n",
    "        return Item(variable, body, dot_position)\n",
    "    \n",
    "    @property\n",
    "    def next_symbol(self):\n",
    "        if self.dot_position == len(self.body):\n",
    "            return None\n",
    "        return self.body[self.dot_position]\n",
    "    \n",
    "\n",
    "Item.from_str(\"V → A B •C D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closure of items\n",
    "Closure of a set of items is the set combined with items that can be obtained\n",
    "by pushing dot from variable into the body of a rule with that variable in its head,\n",
    "e.g. `closure {E → •B} = {E → •B, B → •0, B → •1}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B → •0\n",
      "B → •1\n",
      "E → E * •B\n"
     ]
    }
   ],
   "source": [
    "def closure(grammar, items):\n",
    "    rules = grammar.rules\n",
    "    new_items = set()\n",
    "    for item in items:\n",
    "        variable = item.next_symbol\n",
    "        if variable not in grammar.variables:\n",
    "            continue\n",
    "        for head, body in filter(lambda rule: rule[0] == variable, rules):\n",
    "            new_item = Item(head, body, 0)\n",
    "            if new_item not in items:\n",
    "                new_items.add(new_item)\n",
    "    return closure(grammar, items | new_items) if new_items else items\n",
    "                    \n",
    "for item in sorted(map(str, closure(grammar, {Item.from_str(\"E → E * •B\")}))):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### States (sets of items)\n",
    "The core idea of LR parser is that its states are just sets of possible items.\n",
    "When parser have already read something from input, it doesn't \"know\" yet what\n",
    "rule he is going to apply and what AST node he is going to build,\n",
    "but he does know what items correspond to already read symbols.\n",
    "Actually all possible items corresponding to some state fully specify this state.\n",
    "And the set of all possible sets of items is finite.\n",
    "Thus number of states is finite.\n",
    "And we are going to precompute all the states!\n",
    "\n",
    "With purpose of saving memory a set of items can be represented by items that\n",
    "can't be computed as closure of other items in this set.\n",
    "For example set {`E → E * •B`, `B → •1`, `B → •0`} can be represented by item\n",
    "`E → E * •B` alone.\n",
    "Here is class `ItemSet` that implements such representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item set {B → •0, B → •1, E → E * •B} has kernel items E → E * •B\n"
     ]
    }
   ],
   "source": [
    "@dataclasses.dataclass(frozen=True)\n",
    "class ItemSet:\n",
    "    kernel_items: frozenset[Item]\n",
    "    domain: Grammar\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_items(grammar, items):\n",
    "        # I assume items can be generated by closure() if they have dot at the\n",
    "        # begining of the body.\n",
    "        # It is not true in general case, but sufficient for LR parser states\n",
    "        kernel_items = filter(lambda item: item.dot_position > 0, items)\n",
    "        kernel_items = frozenset(kernel_items)\n",
    "        return ItemSet(kernel_items, grammar)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        yield from sorted(closure(self.domain, self.kernel_items))\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"{\" + \", \".join(sorted(map(str, self))) + \"}\"\n",
    "\n",
    "    def __bool__(self):\n",
    "        return bool(self.kernel_items)\n",
    "\n",
    "items = \"E → E * •B, B → •1, B → •0\".split(\", \")\n",
    "s = ItemSet.from_items(grammar, map(Item.from_str, items))\n",
    "print(\"Item set\", s, \"has kernel items\", \", \".join(map(str, s.kernel_items)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GOTO\n",
    "\n",
    "    GOTO(current_parser_state, next_symbol) -> next_parser_state\n",
    " \n",
    "The GOTO function computes next parser state(item set)\n",
    "based on its current state(item set).\n",
    "Since a state is just a set of items, the function is pretty straightforward:\n",
    "assuming `next_symbol=Y` for every item `W → X •Y Z` from current set of items\n",
    "add `W → X Y• Z` into the next state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goto({B → •0, B → •1, E → E * •B}, \"1\")  =  {B → 1•}\n"
     ]
    }
   ],
   "source": [
    "def goto(grammar, items, next_symbol):\n",
    "    next_items = set()\n",
    "    for item in items:\n",
    "        if item.next_symbol == next_symbol:\n",
    "            next_item = Item(item.variable, item.body, item.dot_position + 1)\n",
    "            next_items.add(next_item)\n",
    "    return ItemSet.from_items(grammar, next_items)\n",
    "print(f'goto({s}, \"1\")  = ', goto(grammar, s, \"1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The states precomputed\n",
    "We can use the functoin `goto()` to precompute all reachable states of parser.\n",
    "With this purpose in mind we will need a starting state, a starting item.\n",
    "We need a rule that will contain a starting symbol in its body.\n",
    "So we augment our grammar with such a rule:\n",
    "we add new start symbol `START` and new rule `START → $ OLD_START $`,\n",
    "where `$` denotes start or end of input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start symbol: START\n",
      "variables: B, E, START\n",
      "terminals: '$', '*', '+', '0', '1'\n",
      "rules:\tB → 0\n",
      "\tB → 1\n",
      "\tE → B\n",
      "\tE → E * B\n",
      "\tE → E + B\n",
      "\tSTART → $ E $\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def augment_grammar(grammar):\n",
    "    variables, rules, start = grammar.variables, grammar.rules, grammar.start\n",
    "    new_start = \"START\"\n",
    "    # if variable START is already in the grammar, we use START' or START'' ...\n",
    "    while new_start in variables:\n",
    "        new_start += \"'\"\n",
    "    new_variables = variables | {new_start}\n",
    "    new_terminals = grammar.terminals | {\"$\"}\n",
    "    new_rules = [(new_start, (\"$\", start, \"$\"))] + rules\n",
    "    return Grammar(new_variables, new_terminals, new_rules, new_start)\n",
    "augmented_grammar = augment_grammar(grammar)\n",
    "print(augmented_grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can precompute all the states that are reachable from item `START → $ •OLD_START $`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: {B → •0, B → •1, E → •B, E → •E * B, E → •E + B, START → $ •E $}\n",
      "1: {B → 0•}\n",
      "2: {B → 1•}\n",
      "3: {E → B•}\n",
      "4: {E → E •* B, E → E •+ B, START → $ E •$}\n",
      "5: {START → $ E $•}\n",
      "6: {B → •0, B → •1, E → E * •B}\n",
      "7: {B → •0, B → •1, E → E + •B}\n",
      "8: {E → E * B•}\n",
      "9: {E → E + B•}\n"
     ]
    }
   ],
   "source": [
    "def grammar_states(grammar):\n",
    "    assert grammar.rules[0][0] == grammar.start\n",
    "    variables, rules, start = grammar.variables, grammar.rules, grammar.start\n",
    "    symbols = sorted(grammar.variables | grammar.terminals)\n",
    "    start_state = ItemSet.from_items(grammar, {Item(*rules[0], 1)})\n",
    "    states = [start_state]\n",
    "    states_lookup = {start_state}\n",
    "    processed_states = 0\n",
    "    while processed_states < len(states):\n",
    "        state = states[processed_states]\n",
    "        processed_states += 1\n",
    "        for symbol in symbols:\n",
    "            new_state = goto(grammar, state, symbol)\n",
    "            if new_state and new_state not in states_lookup:\n",
    "                states_lookup.add(new_state)\n",
    "                states.append(new_state)\n",
    "    return states\n",
    "\n",
    "states = grammar_states(augmented_grammar)\n",
    "for i, state in enumerate(states):\n",
    "    print(f\"{i}: {state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions precomputed\n",
    "We have states precomputed. Cool! Now let's precompute actions that should be executed.\n",
    "For each possible state and each posible terminal on input we will compute desired action.\n",
    "\n",
    "LR parser supports these types of actions:\n",
    "1. SHIFT: push the terminal from input into the stack.\n",
    "2. REDUCE: pack a few symbols from stack into an AST node.\n",
    "3. ACCEPT: accept current stack as succesfully built AST tree. \n",
    "4. DIE: raise an exception if there is no reasanable action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 '$' -> die START \t {B → •0, B → •1, E → •B, E → •E * B, E → •E + B, START → $ •E $}\n",
      "0 '*' -> die START \t {B → •0, B → •1, E → •B, E → •E * B, E → •E + B, START → $ •E $}\n",
      "0 '+' -> die START \t {B → •0, B → •1, E → •B, E → •E * B, E → •E + B, START → $ •E $}\n",
      "0 '0' -> shift 1 \t {B → •0, B → •1, E → •B, E → •E * B, E → •E + B, START → $ •E $}\n",
      "0 '1' -> shift 2 \t {B → •0, B → •1, E → •B, E → •E * B, E → •E + B, START → $ •E $}\n",
      "1 '$' -> reduce B 1 \t {B → 0•}\n",
      "1 '*' -> reduce B 1 \t {B → 0•}\n",
      "1 '+' -> reduce B 1 \t {B → 0•}\n",
      "1 '0' -> reduce B 1 \t {B → 0•}\n",
      "1 '1' -> reduce B 1 \t {B → 0•}\n",
      "2 '$' -> reduce B 1 \t {B → 1•}\n",
      "2 '*' -> reduce B 1 \t {B → 1•}\n",
      "2 '+' -> reduce B 1 \t {B → 1•}\n",
      "2 '0' -> reduce B 1 \t {B → 1•}\n",
      "2 '1' -> reduce B 1 \t {B → 1•}\n",
      "3 '$' -> reduce E 1 \t {E → B•}\n",
      "3 '*' -> reduce E 1 \t {E → B•}\n",
      "3 '+' -> reduce E 1 \t {E → B•}\n",
      "3 '0' -> reduce E 1 \t {E → B•}\n",
      "3 '1' -> reduce E 1 \t {E → B•}\n",
      "4 '$' -> accept E \t {E → E •* B, E → E •+ B, START → $ E •$}\n",
      "4 '*' -> shift 6 \t {E → E •* B, E → E •+ B, START → $ E •$}\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "def precompute_actions(grammar):\n",
    "    states, terminals = grammar_states(grammar), sorted(grammar.terminals)\n",
    "    actions = {}\n",
    "    for i, state in enumerate(states):\n",
    "        kernel_item = next(iter(state.kernel_items))\n",
    "        variable, body = kernel_item.variable, kernel_item.body\n",
    "        for next_terminal in sorted(grammar.terminals):\n",
    "            actions[i, next_terminal] = (\"die\", variable)\n",
    "        for next_terminal in sorted(grammar.terminals - {'$'}):\n",
    "            next_state = goto(grammar, state, next_terminal)\n",
    "            if next_state:\n",
    "                actions[i, next_terminal] = (\"shift\", states.index(next_state))\n",
    "        if any(i.variable == grammar.start and i.dot_position == 2 for i in state.kernel_items):\n",
    "            actions[i, \"$\"] = (\"accept\", variable)\n",
    "        if kernel_item.dot_position == len(kernel_item.body):\n",
    "            for next_terminal in sorted(grammar.terminals):\n",
    "                actions[i, next_terminal] = (\"reduce\", variable, len(body))\n",
    "    return actions\n",
    "\n",
    "actions = precompute_actions(augmented_grammar)\n",
    "for situation, action in list(actions.items())[:22]:\n",
    "    state_index, terminal = situation\n",
    "    print(state_index, repr(terminal), \"->\", *action, \"\\t\", states[state_index])\n",
    "if len(actions) > 22: print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- '$' '*' '+' '0' '1'\n",
      " 0 dST dST dST s1  s2  \t {B → •0, B → •1, E → •B, E → •E * B, E → •E + B, START → $ •E $}\n",
      " 1 rB  rB  rB  rB  rB  \t {B → 0•}\n",
      " 2 rB  rB  rB  rB  rB  \t {B → 1•}\n",
      " 3 rE  rE  rE  rE  rE  \t {E → B•}\n",
      " 4 aE  s6  s7  dE  dE  \t {E → E •* B, E → E •+ B, START → $ E •$}\n",
      " 5 rST rST rST rST rST \t {START → $ E $•}\n",
      " 6 dE  dE  dE  s1  s2  \t {B → •0, B → •1, E → E * •B}\n",
      " 7 dE  dE  dE  s1  s2  \t {B → •0, B → •1, E → E + •B}\n",
      " 8 rE  rE  rE  rE  rE  \t {E → E * B•}\n",
      " 9 rE  rE  rE  rE  rE  \t {E → E + B•}\n"
     ]
    }
   ],
   "source": [
    "table = [[\"   \"] * (len(augmented_grammar.terminals)) for i in range(len(states))]\n",
    "terminals = sorted(augmented_grammar.terminals)\n",
    "for situation, action in list(actions.items()):\n",
    "    state_index, terminal = situation\n",
    "    table[state_index][terminals.index(terminal)] = f\"{action[0][0]}{str(action[1])[:2]:2}\"\n",
    "print(\"--\", \" \".join(map(repr, terminals)))\n",
    "for i, row in enumerate(table):\n",
    "    print(f\"{i:2}\", \" \".join(row), \"\\t\", states[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gotos precomputed\n",
    "We precomputed the actions, why not precompute goto(...) results? We need them for nonterminals to know\n",
    "what state to go when reducing something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 'B'): 3, (0, 'E'): 4, (6, 'B'): 8, (7, 'B'): 9}\n"
     ]
    }
   ],
   "source": [
    "def precompute_gotos(grammar):\n",
    "    states = grammar_states(grammar)\n",
    "    gotos = {}\n",
    "    for i, state in enumerate(states):\n",
    "        for variable in sorted(grammar.variables):\n",
    "            next_state = goto(grammar, state, variable)\n",
    "            if next_state:\n",
    "                gotos[i, variable] = states.index(next_state)\n",
    "    return gotos\n",
    "gotos = precompute_gotos(augmented_grammar)\n",
    "print(gotos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime of the parser\n",
    "Using all the precomputed information we can now write the parser, that uses only numbers/indexes of states, not the states itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'E'\n",
      " ├'B'\n",
      " │ ├'E'\n",
      " │ │ ├'B'\n",
      " │ │ │ ├'1'\n",
      " │ ├'+'\n",
      " │ ├'1'\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def parser(actions, gotos, DEBUG=False):\n",
    "    def parse(source):\n",
    "        stack = [(\"$\", 0)]\n",
    "        i, tokens = 0, list(source) + [\"$\"]\n",
    "        while True:\n",
    "            token = tokens[i]\n",
    "            last_token, state = stack[-1]\n",
    "            action = actions[state, token] \n",
    "            if DEBUG: print(stack, \"<<<\", repr(token), \":\", action)\n",
    "            if action[0] == \"shift\":\n",
    "                next_state, i = action[1], i + 1\n",
    "                stack.append(([token], next_state))\n",
    "            elif action[0] == \"reduce\":\n",
    "                variable, size = action[1:]\n",
    "                stack, node = stack[:-size], stack[size:]\n",
    "                next_state = gotos[stack[-1][1], variable]\n",
    "                stack.append(([variable] + [n for n, _ in node], next_state))\n",
    "                #stack.append((variable, next_state))\n",
    "            elif action[0] == \"accept\":\n",
    "                return stack[1][0]\n",
    "            else:\n",
    "                raise Exception(f\"{repr(stack)} <<< {token}\")\n",
    "    return parse\n",
    "\n",
    "def print_ast(ast, offset = 0):\n",
    "    head, children = ast[0], ast[1:]\n",
    "    print(\" │\" * (offset - 1) + \" ├\" * bool(offset) + repr(head))\n",
    "    for child in children:\n",
    "        print_ast(child, offset + 1)\n",
    "\n",
    "parse = parser(actions, gotos)\n",
    "print_ast(parse(\"1+1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'E'\n",
      " ├'B'\n",
      " │ ├'E'\n",
      " │ │ ├'B'\n",
      " │ │ │ ├'E'\n",
      " │ │ │ │ ├'B'\n",
      " │ │ │ │ │ ├'1'\n",
      " │ │ │ ├'*'\n",
      " │ │ │ ├'0'\n",
      " │ ├'+'\n",
      " │ ├'1'\n"
     ]
    }
   ],
   "source": [
    "print_ast(parse(\"1*0+1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'E'\n",
      " ├'B'\n",
      " │ ├'1'\n"
     ]
    }
   ],
   "source": [
    "print_ast(parse(\"1\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
