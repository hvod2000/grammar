{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeterministicNone:\n",
    "    def __repr__(self): return \"NONE\"\n",
    "    def __bool__(self): return False\n",
    "    def __hash__(self): return 52604\n",
    "NONE = DeterministicNone() # This is small hack to have reproducible jupyter outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR parser\n",
    "This notebook contains both theory and implementation of LR(0) parser according to the\n",
    "[Dragon Book](https://en.wikipedia.org/wiki/Compilers:_Principles,_Techniques,_and_Tools).\n",
    "\n",
    "LR parser is a bottom-up parser that can parse context-free languages in linear time.\n",
    "It reads input tokens, concatenates them into AST nodes in hope that\n",
    "at the end the whole input will collapse into one big AST node, which will be the AST itself.\n",
    "If you read this notebook in hopes of understanding LR parser,\n",
    "please make sure you have already understood what is\n",
    "[CFG](https://en.wikipedia.org/wiki/Context-free_grammar) and\n",
    "[AST](https://en.wikipedia.org/wiki/Abstract_syntax_tree),\n",
    "since I wan't cover them here.\n",
    "\n",
    "\n",
    "### LR(0) parser\n",
    "LR(0) parser is the simplest one. It's also sometimes called SLR.\n",
    "* S - Simple. But I can't call it simple: it's much more complex than PEG or LL.\n",
    "* L - Left-to-right: the parser reads an input from left to right without peeking at the end of the input.\n",
    "* R - Rightmost derivation in reverse: the parser builds tree by operating on the right end of list of nodes.\n",
    "\n",
    "I don't yet understand what zero in parenthes means, I will find it out when I will be writing LR(1) parser.\n",
    "\n",
    "This notebook contains the theory and the code of LR(0) parser\n",
    "splitted into bunch of sections. Every section consists of **header**, description and `the code itself`.\n",
    "But before that I will tell you the core idea of LR parser:\n",
    "> To build LR parser one should take finite automaton of LL parser with conflicts and\n",
    "resolve them by transformating this this nondeterministic finite automaton into deterministic one.\n",
    "Obtained finite automaton is the LR parser.\n",
    "\n",
    "I don't expect anyone to understand what I just wrote,\n",
    "but for me that description of the parser have divided my life into before and after:\n",
    "before I understood the thing and after. So I had to include it in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example context-free grammar\n",
    "Before doing any kind of experiments with grammars, we need a lab rat.\n",
    "For that purpose I have copy-pasted rules of a CFG grammar from [wikipedia](https://en.wikipedia.org/wiki/Context-free_grammar#Well-formed_parentheses).\n",
    "# TODO\n",
    "But I don't force you to use it:\n",
    "you can change the variable `grammar_source` to your own lab rat and see if the experiment give the same outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_source = \"\"\"\n",
    "    S → S U\n",
    "    S → U\n",
    "    U → ( U )\n",
    "    U → ( )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These rules correspond to context-free grammar,\n",
    "that desribes context-free language that contains these sentences:\n",
    "    \n",
    "    (), (()), ()(), (()()), ((())()), (()(()(()))), ()()()()()(), (())((()))(((())))\n",
    "\n",
    "In other words, this is the grammar of well-formed parentheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing grammar rules\n",
    "Plain text rules are cool, but we need to represent them with some kind of data structure.\n",
    "I will NamedTuple for that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S → S U\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "Rule = namedtuple(\"Rule\", [\"head\", \"body\"])\n",
    "Rule.__str__ = lambda rule: rule[0] + \" → \" + \" \".join(rule[1])\n",
    "print(Rule(\"S\", (\"S\", \"U\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser of rules\n",
    "Everyone knows that to write a parser you have to write a parser.\n",
    "So here is the code of a parser of grammar rules.\n",
    "We need this parser to translate our lab rat into list of rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Rule(head='S', body=('S', 'U')),\n",
       " Rule(head='S', body=('U',)),\n",
       " Rule(head='U', body=('(', 'U', ')')),\n",
       " Rule(head='U', body=('(', ')')))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_rules(source):\n",
    "    for rule in source.strip().split(\"\\n\"):\n",
    "        head, body = rule.strip().split(\" → \")\n",
    "        yield Rule(head, tuple(body.split(\" \")))\n",
    "rules = tuple(parse_rules(grammar_source))\n",
    "rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive variables, terminals, start symbol from rules\n",
    "Mathematically speaking we [should](https://en.wikipedia.org/wiki/Context-free_grammar#Formal_definitions) specify variables, terminals, rules and start symbol in order to call it a grammar,\n",
    "but we(programmer) are too lazy for this.\n",
    "Why write all this when you can simply extract all the information you need solely from the rules of grammar.\n",
    "So instead of specifying all these things I wrote a function `derive_symbols()`\n",
    "to derive terminals and variables from grammar rules.\n",
    "The idea of derivation is based on the fact, that a variable can be rule head,\n",
    "while a terminal may occure only in the body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables: 'U', 'S'\n",
      "terminals: ')', '('\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "def derive_symbols(rules):\n",
    "    variables = {variable for variable, body in rules}\n",
    "    terminals = {t for _, body in rules for t in body if t not in variables}\n",
    "    return frozenset(variables), frozenset(terminals)\n",
    "variables, terminals = derive_symbols(rules)\n",
    "print(\"variables: \" + \", \".join(map(repr, variables)))\n",
    "print(\"terminals: \" + \", \".join(map(repr, terminals)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will assume the head of the first rule to be the start symbol.\n",
    "this assumption is based solely on the fact that it is true for **my** lab rat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_symbol = rules[0].head\n",
    "start_symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammar representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am too lazy to bring all four variables(variables, terminals, rules, start symbol) everywhere where I need them,\n",
    "so it makes sence to implement a `Grammar` class according to its [mathematical definition](https://en.wikipedia.org/wiki/Context-free_grammar#Formal_definitions).\n",
    "However I am also too lazy to write the whole class myself, so instead I will once again use named tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Grammar(variables=frozenset({'U', 'S'}), terminals=frozenset({')', '('}), rules=(Rule(head='S', body=('S', 'U')), Rule(head='S', body=('U',)), Rule(head='U', body=('(', 'U', ')')), Rule(head='U', body=('(', ')'))), start_symbol='S')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Grammar(namedtuple(\"Grammar\", \"variables terminals rules start_symbol\")):\n",
    "    @staticmethod\n",
    "    def from_source(source):\n",
    "        rules = tuple(parse_rules(source))\n",
    "        variables, terminals = derive_symbols(rules)\n",
    "        start_symbol = rules[0].head\n",
    "        return Grammar(variables, terminals, rules, start_symbol)\n",
    "\n",
    "Grammar(variables, terminals, rules, start_symbol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since grammar is the most important class in this notebook.\n",
    "Thus we should grant him a pretty `__str__` and `_repr_pretty_` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "start:\tS\n",
       "variables: U S\n",
       "terminals: ) (\n",
       "rules:\tS → S U\n",
       "\tS → U\n",
       "\tU → ( U )\n",
       "\tU → ( )"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grammar_to_str(grammar):\n",
    "    return (\"start:\\t\" + grammar.start_symbol\n",
    "        + \"\\nvariables: \" + \" \".join(map(str, grammar.variables))\n",
    "        + \"\\nterminals: \" + \" \".join(map(str, grammar.terminals))\n",
    "        + \"\\nrules:\\t\" + \"\\n\\t\".join(map(str, grammar.rules)))\n",
    "Grammar.__str__ = grammar_to_str\n",
    "Grammar._repr_pretty_ = lambda grammar, p, _: p.text(str(grammar))\n",
    "\n",
    "Grammar(variables, terminals, rules, start_symbol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THEORY\n",
    "In section I will try to explain the theory behind LR parsers.\n",
    "I will try really hard to explain the thing to you. Please, be dead serious.\\\n",
    "P.S. english is not my native language, so I don't know who is \"dead serious\", I know only \"dead morose\".\n",
    "\n",
    "Imagine that you are a racoon.\n",
    "You know... the one that lives in a coffee machine and makes coffee.\n",
    "It's hard for me to imagine, since I have never seen a coffee machine.\n",
    "Let's assume that you(a racoon) have decided to quit.\n",
    "You no longer make coffee, now you are a parser.\n",
    "You parse for living.\n",
    "Parsing is not what your mother wished for you,\n",
    "but it's a well-paying job and you like it.\\\n",
    "A client silently gives you a **grammar** and a stack of green banknotes.\n",
    "A few days later owls bring you **tokens**: one per owl.\n",
    "You're sitting at home and building the **tree** out of tokens.\n",
    "You should be fast, because there are more and more owls.\n",
    "The last one of them will gives you the last token and takes the tree from you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you get a grammar with rules `S → + T S`, `S → * T S` `S → T`, `T → 0`, `T → 1`.\\\n",
    "You know right away what it is.\n",
    "It is a grammar of [reverse reverse polish notation](https://en.wikipedia.org/wiki/Polish_notation).\n",
    "There is no one at home to appreciate such an ingenious insight, but you are ok with it.\n",
    "You are already used to freelancing: you are alone at while your family is somewhere else... \\\n",
    "**Owls are approaching!**\\\n",
    "You prepare yourself. You know the job is to build the node, that corresponds to start symbol, which is `S`.\n",
    "And you know what rules can be used to build it: `S → + T S`, `S → * T S` `S → T`.\\\n",
    "You receive the first token: it's `+`.\n",
    "So you know that you are not going to use `S → * T S` or `S → T`, since they don't have `+` at the beginning.\n",
    "You will use the rule `S → + T S`, which tells you that after `+` you should receive something\n",
    "that can be used to assemble `T`: `0` or `1`. \\\n",
    "You recive the second token: it's `1`. You use it to build AST node corresponding to `T`.\n",
    "And you put it(the node) on your desk near the `+`.\n",
    "Now you are expecting something to build `S` node. \\\n",
    "Aaand... you recive the last token: it's `0`. You use it to build AST node corresponding to `T`.\n",
    "Then you use the node `T` to build AST node corresponding to `S`.\n",
    "You put `S` near `T` and `+` to fold it using rule `S → + T S`. After folding you have the AST tree:\n",
    "\n",
    "      S\n",
    "     /|\\\n",
    "    + T S\n",
    "      | |\n",
    "      1 T\n",
    "        |\n",
    "        0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After each owl arrival you know exactly what rule you sould apply. you are precise and untroubled.\n",
    "And nothing can disturb you. Nothing except the next grammar: \n",
    "\n",
    "    S → S S\n",
    "    S → ( S )\n",
    "    S → ( )\n",
    "\n",
    "When you recive the first token `(` you know nothing.\n",
    "You don't know what rule you should apply because every one of them has `(` at the beginning.\n",
    "Maybe you should apply third rule and expect the ending `)`:\n",
    "\n",
    "    S → (•)\n",
    "         └── you are here\n",
    "\n",
    "Or maybe you should apply second rule and expect `(` as next token:\n",
    "\n",
    "    S → (•S )\n",
    "         └── you are here\n",
    "\n",
    "Also it's possible to inside the first `S` of the body of the first rule:\n",
    "\n",
    "    S → S S\n",
    "        └── you are inside that S\n",
    "        S → (•S )\n",
    "             ├── you are here or there\n",
    "        S → (•)\n",
    "\n",
    "You are confused since you don't know what rule to choose.\n",
    "So you decide to write down received token and\n",
    "all possible places(inside rules) where you may be right now:\n",
    "    \n",
    "    \"(\"  S → (•S )  S → (•)\n",
    "\n",
    "After receiving a few more `(`, you have written this line:\n",
    "\n",
    "    \"(\" S→(•S),S→(•)    \"(\" S→(•S),S→(•)    \"(\" S→(•S),S→(•)\n",
    "\n",
    "You receive `)` and now you can clearly see that from places like `S→(•S)` and `S→(•)`\n",
    "you could get only into `S → ( )•`.\n",
    "So now you know precisely that you are here: `S → ( )•`.\n",
    "Thus you use the rule `S → ( )` to build symbol `S` and get into this state:\n",
    "\n",
    "    \"(\" S→(•S),S→(•)    \"(\" S→(•S),S→(•)    S  S→(S•)\n",
    "                                                   └── we got here from  S→(•S)\n",
    "\n",
    "A few minutes later you recieve `)`, so you use it to build new `S` out of `(`, `S`, `)`:\n",
    "\n",
    "    \"(\" S→(•S),S→(•)     S  S→(S•)\n",
    "\n",
    "Finally you recieve last token `)` and use it put everything into one big `S`.\n",
    "\n",
    "**TO BE CONTINUED...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Items\n",
    "Positions within the rules are called LR items.\n",
    "In other words LR items are just rules with dot in body, e.g. `E → E •+ B`, `S → S •S`, `S → ( )•`.\n",
    "Items indicate that the parser has recognized a string correspondig to the part of rule before the dot,\n",
    "e.g. `E → E * •B` means that the parser has recognized `E` and `*` on the input and now expects to read `B`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S → S • U\n"
     ]
    }
   ],
   "source": [
    "class Item(namedtuple(\"Item\", \"rule dot_position\")):\n",
    "    def __str__(item):\n",
    "        body = list(item.rule.body)\n",
    "        body.insert(item.dot_position, \"•\")\n",
    "        return str(item.rule.head) + \" → \" + \" \".join(map(str, body))\n",
    "\n",
    "print(Item(rule=rules[0], dot_position=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operation of taking the next symbol after the dot will be very useful for us in later stages.\n",
    "So it makes sense to implement it as a method of the `Item` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_symbol_after_the_dot(item):\n",
    "    if item.dot_position < len(item.rule.body):\n",
    "        return item.rule.body[item.dot_position]\n",
    "    return None\n",
    "Item.next_symbol = property(get_next_symbol_after_the_dot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closure of items\n",
    "Closure of a set of items is the set combined with items that can be obtained\n",
    "by pushing the dot from the head into the body of a rule,\n",
    "e.g.\n",
    "    \n",
    "    closure of S → ( •S ) =\n",
    "        S → ( •S )\n",
    "        S → •S S\n",
    "        S → •( S )\n",
    "        S → •( )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    closure of S → S • U =\n",
      "\tS → S • U\n",
      "\tU → • ( )\n",
      "\tU → • ( U )\n"
     ]
    }
   ],
   "source": [
    "def close(grammar, items):\n",
    "    closure, rules = set(items), grammar.rules\n",
    "    for variable in (item.next_symbol for item in items):\n",
    "        closure |= {Item(rule, 0) for rule in rules if rule.head == variable}\n",
    "    return close(grammar, closure) if closure > items else frozenset(closure)\n",
    "                    \n",
    "grammar = Grammar(variables, terminals, rules, start_symbol)\n",
    "item = Item(rule=rules[0], dot_position=1)\n",
    "closure = close(grammar, {item})\n",
    "print(f\"    closure of {item} =\\n\\t\"+\"\\n\\t\".join(map(str, closure)))\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Btw, it's quite convenient to have this functionality as part of grammar class.\n",
    "So I will bind it to the class as a method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grammar.close = close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### States (sets of items)\n",
    "The core idea of LR parser is that its states are just sets of possible items.\n",
    "When parser have already read something from input, it doesn't \"know\" yet what\n",
    "rule he is going to apply and what AST node he is going to build,\n",
    "but he does know what items correspond to already read symbols.\n",
    "Actually all possible items corresponding to some state fully specify this state.\n",
    "And the set of all possible sets of items is finite.\n",
    "Thus number of states is finite.\n",
    "And we are going to precompute all the states.\n",
    "But before all that we need a class for set of items.\n",
    "\n",
    "With purpose of saving memory a set of items can be represented by items that\n",
    "can't be computed as closure of other items in this set.\n",
    "For example set {`E → E * •B`, `B → •1`, `B → •0`} can be represented by item\n",
    "`E → E * •B` alone, since items `B → •1`, `B → •0` can be obtained by finding closure of `E → E * •B`.\n",
    "So the rule `E → E * •B` is a **kernel item** of set {`E → E * •B`, `B → •1`, `B → •0`}.\n",
    "\n",
    "How to understand which items are kernel items?\\\n",
    "In general case it's quite complex and requires topological sort of rules...\n",
    "However, according to the Dragon Book, only a small subset of item sets appears during parsing,\n",
    "and all of them have kernel items with non-zero dot position.\n",
    "In other words we can just compare dot position with zero to find out if item is a kernel item.\n",
    "\n",
    "Here is the class `ItemSet` that implements memory effective representation of item sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set {S → S • U, U → • ( ), U → • ( U )} has kernel items S → S • U\n"
     ]
    }
   ],
   "source": [
    "class ItemSet(namedtuple(\"ItemSet\", \"grammar kernel_items\")):\n",
    "    @staticmethod\n",
    "    def from_items(grammar, items):\n",
    "        kernel_items = {item for item in items if item.dot_position > 0}\n",
    "        return ItemSet(grammar, frozenset(kernel_items))\n",
    "    \n",
    "    def __iter__(self):\n",
    "        yield from self.grammar.close(self.kernel_items)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"{\" + \", \".join(sorted(map(str, self))) + \"}\"\n",
    "\n",
    "    def __bool__(self):\n",
    "        return bool(self.kernel_items)\n",
    "################################################################################\n",
    "items = ItemSet.from_items(grammar, {item})\n",
    "print(f\"Set {items} has kernel items\", \", \".join(map(str, items.kernel_items)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GOTO\n",
    "\n",
    "    GOTO(current_parser_state, next_symbol) -> next_parser_state\n",
    " \n",
    "The GOTO function computes next parser state(item set)\n",
    "based on its current state(item set).\n",
    "Since a state is just a set of items, the function is pretty straightforward:\n",
    "assuming `next_symbol=Y` for every item `W → X •Y Z` from current set of items\n",
    "add `W → X Y• Z` into the next state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goto({S → S • U, U → • ( ), U → • ( U )}, \"(\")  =\n",
      "\t {U → ( • ), U → ( • U ), U → • ( ), U → • ( U )}\n",
      "goto({U → ( • ), U → ( • U ), U → • ( ), U → • ( U )}, \")\")  =\n",
      "\t {U → ( ) •}\n"
     ]
    }
   ],
   "source": [
    "def goto(grammar, items, next_symbol):\n",
    "    next_items = set()\n",
    "    for item in items:\n",
    "        if item.next_symbol == next_symbol:\n",
    "            next_items.add(Item(item.rule, item.dot_position + 1))\n",
    "    return ItemSet.from_items(grammar, next_items)\n",
    "################################################################################\n",
    "Grammar.goto = goto\n",
    "print(f'goto({items}, \"(\")  =\\n\\t', items2 := grammar.goto(items, \"(\"))\n",
    "print(f'goto({items2}, \")\")  =\\n\\t', grammar.goto(items2, \")\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The states precomputed\n",
    "We can use the functoin `goto()` to precompute all reachable states of parser.\n",
    "With this purpose in mind we will need a starting state, a starting item.\n",
    "We need a rule that will contain the whole input in its body.\n",
    "There are several options:\n",
    "\n",
    "1. `NEW_START_SYMBOL → • OLD_START_SYMBOL $`\\\n",
    "This is the starting item used in the Dragon book.\n",
    "`NEW_START_SYMBOL` is some random name that's not going to be used anywhere.\n",
    "And `$` is the end of input.\n",
    "This option has disadvantage compared to option #2:\n",
    "the new rule adds some exceptions to the way we find kernel items...\n",
    "\n",
    "\n",
    "2. `NONE → NONE •START_SYMBOL NONE`\\\n",
    "This is the item I am going to use.\n",
    "I understand it like that:\n",
    "rule without a head contains the whole tree(`START_SYMBOL`) with no terminals before or after.\n",
    "`NONE` in the body denotes start and end of input.\n",
    "\n",
    "So we augment our grammar with such a rule:\n",
    "we add new start symbol `START` and new rule `START → $ OLD_START $`,\n",
    "where `$` denotes start or end of input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: {NONE → NONE • S NONE, S → • S U, S → • U, U → • ( ), U → • ( U )}\n",
      "1: {NONE → NONE S • NONE, S → S • U, U → • ( ), U → • ( U )}\n",
      "2: {S → U •}\n",
      "3: {U → ( • ), U → ( • U ), U → • ( ), U → • ( U )}\n",
      "4: {S → S U •}\n",
      "5: {U → ( U • )}\n",
      "6: {U → ( ) •}\n",
      "7: {U → ( U ) •}\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "def reachable_states(grammar):\n",
    "    symbols = grammar.variables | grammar.terminals\n",
    "    start_item = Item(Rule(NONE, (NONE, grammar[3], NONE)), dot_position=1)\n",
    "    start_state = ItemSet.from_items(grammar, {start_item})\n",
    "    states, unprocessed_states = set(), {start_state}\n",
    "    while unprocessed_states:\n",
    "        state = unprocessed_states.pop()\n",
    "        states.add(state)\n",
    "        yield state\n",
    "        for new_state in (grammar.goto(state, symbol) for symbol in symbols):\n",
    "            if new_state and new_state not in states:\n",
    "                unprocessed_states.add(new_state)\n",
    "\n",
    "states = list(reachable_states(grammar))\n",
    "print(\"\\n\".join(f\"{i}: {state}\" for i, state in enumerate(states)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "States are useful enough to bind them as a property of `Grammar`.\n",
    "Also they are needed too often to be left uncached.\n",
    "Thus I use `functools.cache()` to cache them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: {NONE → NONE • S NONE, S → • S U, S → • U, U → • ( ), U → • ( U )}\n",
      "1: {NONE → NONE S • NONE, S → S • U, U → • ( ), U → • ( U )}\n",
      "2: {S → U •}\n",
      "3: {U → ( • ), U → ( • U ), U → • ( ), U → • ( U )}\n",
      "4: {S → S U •}\n",
      "5: {U → ( U • )}\n",
      "6: {U → ( ) •}\n",
      "7: {U → ( U ) •}\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "get_states = lambda grammar: tuple(reachable_states(grammar))\n",
    "Grammar.states = property(functools.cache(get_states))\n",
    "################################################################################\n",
    "print(\"\\n\".join(f\"{i}: {state}\" for i, state in enumerate(grammar.states)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gotos precomputed\n",
    "We precomputed the states, why not precompute goto(...) results?\n",
    "During runtime we need goto(...) results for nonterminals to know what state to go when reducing something.\n",
    "I will use numbers of states as keys and results instead of states itself,\n",
    "since during runtime I don't want to store the item sets in memory.\n",
    "I will save memory by storing only indexes of sets, not sets themself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goto 0 U -> 2\n",
      "goto 0 S -> 1\n",
      "goto 0 ( -> 3\n",
      "goto 1 U -> 4\n",
      "goto 1 ( -> 3\n",
      "goto 3 ) -> 6\n",
      "goto 3 U -> 5\n",
      "goto 3 ( -> 3\n",
      "goto 5 ) -> 7\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "def precompute_goto(grammar):\n",
    "    indexes = {state:i for i, state in enumerate(grammar.states)}\n",
    "    symbols = grammar.terminals | grammar.variables\n",
    "    gotos = {}\n",
    "    for i, state in enumerate(grammar.states):\n",
    "        for symbol in symbols:\n",
    "            next_state = grammar.goto(state, symbol)\n",
    "            if next_state in indexes:\n",
    "                gotos[i, symbol] = indexes[next_state]\n",
    "    return gotos\n",
    "\n",
    "Grammar.gotos = property(functools.cache(precompute_goto))\n",
    "print(\"\\n\".join(f\"goto {i} {s} -> {j}\" for (i, s), j in grammar.gotos.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions precomputed\n",
    "We have states and gotos precomputed.\n",
    "Cool! Now let's precompute for each state the action that should be executed.\n",
    "For each possible state and each posible terminal on input we will compute desired action.\n",
    "\n",
    "LR parser supports these types of actions:\n",
    "1. SHIFT: push the terminal from input into the stack and enter another state.\\\n",
    "We use this action when the next token can lead us to some valid state.\n",
    "2. REDUCE: pack a few symbols from stack into an AST node and go to another state according to the goto().\\\n",
    "This action should be applied when our current state(set of items)\n",
    "contains only one item and this item has the dot at its end.\n",
    "3. ACCEPT: accept current stack as succesfully built AST tree.\\\n",
    "This action should be performed when we have reаd all the input and\n",
    "have `None → None S • None` in the current set of items.\n",
    "4. DIE: raise an exception if there is no reasanable action\\\n",
    "This action should be performed when there are no other actions available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define function that computes actions for specified state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "def get_actions(grammar, state_index):\n",
    "    state = grammar.states[state_index]\n",
    "    if any(i.dot_position == len(i.rule.body) for i in state.kernel_items):\n",
    "        if len(state.kernel_items) != 1:\n",
    "            raise ValueError(f\"CONFLICT DETECTED: {state}\")\n",
    "        item = next(iter(state.kernel_items))\n",
    "        action = (\"reduce\", item.rule.head, len(item.rule.body))\n",
    "        return {term: action for term in grammar.terminals} | {NONE: action}\n",
    "    actions = {}\n",
    "    for next_terminal in grammar.terminals:\n",
    "        next_state = grammar.gotos.get((state_index, next_terminal), None)\n",
    "        if next_state is not None:\n",
    "            actions[next_terminal] = (\"shift\", next_state)\n",
    "    if any(i.next_symbol == NONE for i in state.kernel_items):\n",
    "        actions[NONE] = \"accept\"\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the function that precoputes the actions for all reachable states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (     ->  ('shift', 3)            {NONE → NONE • S NONE, S → • S U, S → • U, U → • ( ), U → • ( U )}\n",
      "1 (     ->  ('shift', 3)            {NONE → NONE S • NONE, S → S • U, U → • ( ), U → • ( U )}\n",
      "1 NONE  ->  accept                  {NONE → NONE S • NONE, S → S • U, U → • ( ), U → • ( U )}\n",
      "2 )     ->  ('reduce', 'S', 1)      {S → U •}\n",
      "2 (     ->  ('reduce', 'S', 1)      {S → U •}\n",
      "2 NONE  ->  ('reduce', 'S', 1)      {S → U •}\n",
      "3 )     ->  ('shift', 6)            {U → ( • ), U → ( • U ), U → • ( ), U → • ( U )}\n",
      "3 (     ->  ('shift', 3)            {U → ( • ), U → ( • U ), U → • ( ), U → • ( U )}\n",
      "4 )     ->  ('reduce', 'S', 2)      {S → S U •}\n",
      "4 (     ->  ('reduce', 'S', 2)      {S → S U •}\n",
      "4 NONE  ->  ('reduce', 'S', 2)      {S → S U •}\n",
      "5 )     ->  ('shift', 7)            {U → ( U • )}\n",
      "6 )     ->  ('reduce', 'U', 2)      {U → ( ) •}\n",
      "6 (     ->  ('reduce', 'U', 2)      {U → ( ) •}\n",
      "6 NONE  ->  ('reduce', 'U', 2)      {U → ( ) •}\n",
      "7 )     ->  ('reduce', 'U', 3)      {U → ( U ) •}\n",
      "7 (     ->  ('reduce', 'U', 3)      {U → ( U ) •}\n",
      "7 NONE  ->  ('reduce', 'U', 3)      {U → ( U ) •}\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "def precompute_actions(grammar):\n",
    "    actions = {}\n",
    "    for i, state in enumerate(grammar.states):\n",
    "        actions.update({(i, t): a for t, a in get_actions(grammar, i).items()})\n",
    "    return actions\n",
    "\n",
    "Grammar.actions = property(functools.cache(precompute_actions))\n",
    "for (i, symbol), action in grammar.actions.items():\n",
    "    state = str(grammar.states[i])\n",
    "    print(f\"{i} {symbol}\".ljust(8) + f\"->  {action}\".ljust(28) + state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR table\n",
    "Paser table is a table that contains all the information needed during runtime.\n",
    "In our case the table should contain all the actions precomputed and some of the gotos(...).\n",
    "We need goto(...) results for nonoterminals,\n",
    "since we will use them to find out which state to go to after reducing something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gotos = grammar.gotos.items()\n",
    "gotos = {(s, t): ns for (s, t), ns in gotos if t in grammar.variables}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime of the parser\n",
    "Using all the precomputed information we can now write the parser, that uses only numbers/indexes of states, not the states itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('S', ('U', '(', ('U', '(', ')'), ')'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################\n",
    "def parse(actions, gotos, tokens, i=0, get_token_type=lambda s: s):\n",
    "    stack = [(NONE, 0)]\n",
    "    while True:\n",
    "        token = get_token_type(tokens[i] if i < len(tokens) else NONE)\n",
    "        previous_token, state = stack[-1]\n",
    "        action = actions.get((state, token), \"die\")\n",
    "        match action:\n",
    "            case \"shift\", next_state:\n",
    "                stack.append((tokens[i], next_state))\n",
    "                i += 1\n",
    "            case \"reduce\", head, body_size:\n",
    "                body = tuple(node for node, _ in stack[-body_size:])\n",
    "                del stack[-body_size:]\n",
    "                stack.append(((head,) + body, gotos[stack[-1][1], head]))\n",
    "            case \"accept\":\n",
    "                return stack[-1][0]\n",
    "            case \"die\":\n",
    "                error_msg = f':( Died at token #{i} \"{token}\" in state {state}'\n",
    "                raise Exception(error_msg)\n",
    "\n",
    "ast = parse(grammar.actions, gotos, \"(())\")\n",
    "ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the parser parsed the parentheses and returned much more parentheses.\n",
    "The parser loves to parse parenthes, but human brains don't.\n",
    "Let's write a small helper function to debug our AST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S\n",
      "├─U\n",
      "│ ├─'('\n",
      "│ ├─U\n",
      "│ │ ├─'('\n",
      "│ │ ├─')'\n",
      "│ ├─')'\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "def print_ast(ast, offset=0, token_printer=None):\n",
    "    match ast:\n",
    "        case tuple((head, *children)):\n",
    "            print(\"│ \" * (offset - 1) + \"├─\" * bool(offset) + str(head))\n",
    "            for child in children:\n",
    "                print_ast(child, offset + 1)\n",
    "        case token:\n",
    "            if token_printer:\n",
    "                token_printer(token, offset = offset * 2)\n",
    "            else:\n",
    "                print(\"│ \" * (offset - 1) + \"├─\" + repr(token))\n",
    "print_ast(ast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S\n",
      "├─S\n",
      "│ ├─S\n",
      "│ │ ├─U\n",
      "│ │ │ ├─'('\n",
      "│ │ │ ├─')'\n",
      "│ ├─U\n",
      "│ │ ├─'('\n",
      "│ │ ├─')'\n",
      "├─U\n",
      "│ ├─'('\n",
      "│ ├─U\n",
      "│ │ ├─'('\n",
      "│ │ ├─U\n",
      "│ │ │ ├─'('\n",
      "│ │ │ ├─')'\n",
      "│ │ ├─')'\n",
      "│ ├─')'\n"
     ]
    }
   ],
   "source": [
    "print_ast(parse(grammar.actions, gotos, \"()()((()))\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE PARSER\n",
    "So the parser is just executing precomputed actions, which he takes from precomputed table.\n",
    "Taking value from table or executing an action is constant time.\n",
    "Therefore parsing is pretty fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(grammar, get_token_type=lambda s:s):\n",
    "    actions = grammar.actions\n",
    "    gotos = grammar.gotos.items()\n",
    "    gotos = {(s, t): ns for (s, t), ns in gotos if t in grammar.variables}\n",
    "    return lambda tokens, i=0: parse(actions, gotos, tokens, i, get_token_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S\n",
      "├─S\n",
      "│ ├─U\n",
      "│ │ ├─'('\n",
      "│ │ ├─')'\n",
      "├─U\n",
      "│ ├─'('\n",
      "│ ├─')'\n"
     ]
    }
   ],
   "source": [
    "parentheses_parser = parser(grammar)\n",
    "print_ast(parentheses_parser(\"()()\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S\n",
      "├─S\n",
      "│ ├─S\n",
      "│ │ ├─U\n",
      "│ │ │ ├─'('\n",
      "│ │ │ ├─U\n",
      "│ │ │ │ ├─'('\n",
      "│ │ │ │ ├─')'\n",
      "│ │ │ ├─')'\n",
      "│ ├─U\n",
      "│ │ ├─'('\n",
      "│ │ ├─')'\n",
      "├─U\n",
      "│ ├─'('\n",
      "│ ├─')'\n"
     ]
    }
   ],
   "source": [
    "print_ast(parentheses_parser(\"(())()()\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
