{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR parser\n",
    "This notebook contains both theory and implementation of LR(0) parser according to the\n",
    "[Dragon Book](https://en.wikipedia.org/wiki/Compilers:_Principles,_Techniques,_and_Tools).\n",
    "\n",
    "LR parser is a bottom-up parser that can parse context-free languages in linear time.\n",
    "It reads input tokens, concatenates them into AST nodes in hope that\n",
    "at the end the whole input will collapse into one big AST node, which will be the AST itself.\n",
    "If you read this notebook in hopes of understanding LR parser,\n",
    "please make sure you have already understood what is\n",
    "[CFG](https://en.wikipedia.org/wiki/Context-free_grammar) and\n",
    "[AST](https://en.wikipedia.org/wiki/Abstract_syntax_tree),\n",
    "since I wan't cover them here.\n",
    "\n",
    "\n",
    "### LR(0) parser\n",
    "LR(0) parser is the simplest one. It's also sometimes called SLR.\n",
    "* S - Simple. But I can't call it simple: it's much more complex than PEG or LL.\n",
    "* L - Left-to-right: the parser reads an input from left to right without peeking at the end of the input.\n",
    "* R - Rightmost derivation in reverse: the parser builds tree by operating on the right end of list of nodes.\n",
    "\n",
    "I don't yet understand what zero in parenthes means, I will find it out when I will be writing LR(1) parser.\n",
    "\n",
    "This notebook contains the theory and the code of LR(0) parser\n",
    "splitted into bunch of sections. Every section consists of **header**, description and `the code itself`.\n",
    "But before that I will tell you the core idea of LR parser:\n",
    "> To build LR parser one should take finite automaton of LL parser with conflicts and\n",
    "resolve them by transformating this this nondeterministic finite automaton into deterministic one.\n",
    "Obtained finite automaton is the LR parser.\n",
    "\n",
    "I don't expect anyone to understand what I just wrote,\n",
    "but for me that description of the parser have divided my life into before and after:\n",
    "before I understood the thing and after. So I had to include it in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example context-free grammar\n",
    "Before doing any kind of experiments with grammars, we need a lab rat.\n",
    "For that purpose I have copy-pasted rules of a CFG grammar from [wikipedia](https://en.wikipedia.org/wiki/Context-free_grammar#Well-formed_parentheses).\n",
    "But I don't force you to use it:\n",
    "you can change the variable `grammar_source` to your own lab rat and see if the experiment give the same outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_source = \"\"\"\n",
    "    S → S S\n",
    "    S → ( S )\n",
    "    S → ( )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These rules correspond to context-free grammar,\n",
    "that desribes context-free language that contains these sentences:\n",
    "    \n",
    "    (), (()), ()(), (()()), ((())()), (()(()(()))), ()()()()()(), (())((()))(((())))\n",
    "\n",
    "In other words, this is the grammar of well-formed parentheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing grammar rules\n",
    "Plain text rules are cool, but we need to represent them with some kind of data structure.\n",
    "I will NamedTuple for that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S → S S\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "Rule = namedtuple(\"Rule\", [\"head\", \"body\"])\n",
    "Rule.__str__ = lambda rule: rule[0] + \" → \" + \" \".join(rule[1])\n",
    "print(Rule(\"S\", (\"S\", \"S\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser of rules\n",
    "Everyone knows that to write a parser you have to write a parser.\n",
    "So here is the code of a parser of grammar rules.\n",
    "We need this parser to translate our lab rat into list of rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rule(head='S', body=('S', 'S')),\n",
       " Rule(head='S', body=('(', 'S', ')')),\n",
       " Rule(head='S', body=('(', ')'))]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_rules(source):\n",
    "    for rule in source.strip().split(\"\\n\"):\n",
    "        head, body = rule.strip().split(\" → \")\n",
    "        yield Rule(head, tuple(body.split(\" \")))\n",
    "rules = list(parse_rules(grammar_source))\n",
    "rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive variables, terminals, start symbol from rules\n",
    "Mathematically speaking we [should](https://en.wikipedia.org/wiki/Context-free_grammar#Formal_definitions) specify variables, terminals, rules and start symbol in order to call it a grammar,\n",
    "but we(programmer) are too lazy for this.\n",
    "Why write all this when you can simply extract all the information you need solely from the rules of grammar.\n",
    "So instead of specifying all these things I wrote a function `derive_symbols()`\n",
    "to derive terminals and variables from grammar rules.\n",
    "The idea of derivation is based on the fact, that a variable can be rule head,\n",
    "while a terminal may occure only in the body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables={'S'}\n",
      "terminals={')', '('}\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "def derive_symbols(rules):\n",
    "    variables = {variable for variable, body in rules}\n",
    "    terminals = {t for _, body in rules for t in body if t not in variables}\n",
    "    return variables, terminals\n",
    "variables, terminals = derive_symbols(rules)\n",
    "print(f\"{variables=}\\n{terminals=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will assume the head of the first rule to be the start symbol.\n",
    "this assumption is based solely on the fact that it is true for **my** lab rat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_symbol = rules[0].head\n",
    "start_symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammar representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am too lazy to bring all four variables(variables, terminals, rules, start symbol) everywhere where I need them,\n",
    "so it makes sence to implement a `Grammar` class according to its [mathematical definition](https://en.wikipedia.org/wiki/Context-free_grammar#Formal_definitions).\n",
    "However I am also too lazy to write the whole class myself, so instead I will once again use named tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Grammar(variables={'S'}, terminals={')', '('}, rules=[Rule(head='S', body=('S', 'S')), Rule(head='S', body=('(', 'S', ')')), Rule(head='S', body=('(', ')'))], start_symbol='S')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################\n",
    "Grammar = namedtuple(\"Grammar\", \"variables terminals rules start_symbol\")\n",
    "Grammar(variables, terminals, rules, start_symbol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since grammar is the most important class in this notebook.\n",
    "Thus we should grant him a pretty `__str__` and `_repr_pretty_` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "start:\tS\n",
       "variables: S\n",
       "terminals: ) (\n",
       "rules:\tS → S S\n",
       "\tS → ( S )\n",
       "\tS → ( )"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grammar_to_str(grammar):\n",
    "    return (\"start:\\t\" + grammar.start_symbol\n",
    "        + \"\\nvariables: \" + \" \".join(map(str, grammar.variables))\n",
    "        + \"\\nterminals: \" + \" \".join(map(str, grammar.terminals))\n",
    "        + \"\\nrules:\\t\" + \"\\n\\t\".join(map(str, grammar.rules)))\n",
    "Grammar.__str__ = grammar_to_str\n",
    "Grammar._repr_pretty_ = lambda grammar, p, _: p.text(str(grammar))\n",
    "\n",
    "Grammar(variables, terminals, rules, start_symbol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THEORY\n",
    "In section I will try to explain the theory behind LR parsers.\n",
    "I will try really hard to explain the thing to you. Please, be dead serious.\\\n",
    "P.S. english is not my native language, so I don't know who is \"dead serious\", I know only \"dead morose\".\n",
    "\n",
    "Imagine that you are a racoon.\n",
    "You know... the one that lives in a coffee machine and makes coffee.\n",
    "It's hard for me to imagine, since I have never seen a coffee machine.\n",
    "Let's assume that you(a racoon) have decided to quit.\n",
    "You no longer make coffee, now you are a parser.\n",
    "You parse for living.\n",
    "Parsing is not what your mother wished for you,\n",
    "but it's a well-paying job and you like it.\\\n",
    "A client silently gives you a **grammar** and a stack of green banknotes.\n",
    "A few days later owls bring you **tokens**: one per owl.\n",
    "You're sitting at home and building the **tree** out of tokens.\n",
    "You should be fast, because there are more and more owls.\n",
    "The last one of them will gives you the last token and takes the tree from you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you get a grammar with rules `S → + T S`, `S → * T S` `S → T`, `T → 0`, `T → 1`.\\\n",
    "You know right away what it is.\n",
    "It is a grammar of [reverse reverse polish notation](https://en.wikipedia.org/wiki/Polish_notation).\n",
    "There is no one at home to appreciate such an ingenious insight, but you are ok with it.\n",
    "You are already used to freelancing: you are alone at while your family is somewhere else... \\\n",
    "**Owls are approaching!**\\\n",
    "You prepare yourself. You know the job is to build the node, that corresponds to start symbol, which is `S`.\n",
    "And you know what rules can be used to build it: `S → + T S`, `S → * T S` `S → T`.\\\n",
    "You receive the first token: it's `+`.\n",
    "So you know that you are not going to use `S → * T S` or `S → T`, since they don't have `+` at the beginning.\n",
    "You will use the rule `S → + T S`, which tells you that after `+` you should receive something\n",
    "that can be used to assemble `T`: `0` or `1`. \\\n",
    "You recive the second token: it's `1`. You use it to build AST node corresponding to `T`.\n",
    "And you put it(the node) on your desk near the `+`.\n",
    "Now you are expecting something to build `S` node. \\\n",
    "Aaand... you recive the last token: it's `0`. You use it to build AST node corresponding to `T`.\n",
    "Then you use the node `T` to build AST node corresponding to `S`.\n",
    "You put `S` near `T` and `+` to fold it using rule `S → + T S`. After folding you have the AST tree:\n",
    "\n",
    "      S\n",
    "     /|\\\n",
    "    + T S\n",
    "      | |\n",
    "      1 T\n",
    "        |\n",
    "        0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After each owl arrival you know exactly what rule you sould apply. you are precise and untroubled.\n",
    "And nothing can disturb you. Nothing except the next grammar: \n",
    "\n",
    "    S → S S\n",
    "    S → ( S )\n",
    "    S → ( )\n",
    "\n",
    "When you recive the first token `(` you know nothing.\n",
    "You don't know what rule you should apply because every one of them has `(` at the beginning.\n",
    "Maybe you should apply third rule and expect the ending `)`:\n",
    "\n",
    "    S → (•)\n",
    "         └── you are here\n",
    "\n",
    "Or maybe you should apply second rule and expect `(` as next token:\n",
    "\n",
    "    S → (•S )\n",
    "         └── you are here\n",
    "\n",
    "Also it's possible to inside the first `S` of the body of the first rule:\n",
    "\n",
    "    S → S S\n",
    "        └── you are inside that S\n",
    "        S → (•S )\n",
    "             ├── you are here or there\n",
    "        S → (•)\n",
    "\n",
    "You are confused since you don't know what rule to choose.\n",
    "So you decide to write down received token and\n",
    "all possible places(inside rules) where you may be right now:\n",
    "    \n",
    "    \"(\"  S → (•S )  S → (•)\n",
    "\n",
    "After receiving a few more `(`, you have written this line:\n",
    "\n",
    "    \"(\" S→(•S),S→(•)    \"(\" S→(•S),S→(•)    \"(\" S→(•S),S→(•)\n",
    "\n",
    "You receive `)` and now you can clearly see that from places like `S→(•S)` and `S→(•)`\n",
    "you could get only into `S → ( )•`.\n",
    "So now you know precisely that you are here: `S → ( )•`.\n",
    "Thus you use the rule `S → ( )` to build symbol `S` and get into this state:\n",
    "\n",
    "    \"(\" S→(•S),S→(•)    \"(\" S→(•S),S→(•)    S  S→(S•)\n",
    "                                                   └── we got here from  S→(•S)\n",
    "\n",
    "A few minutes later you recieve `)`, so you use it to build new `S` out of `(`, `S`, `)`:\n",
    "\n",
    "    \"(\" S→(•S),S→(•)     S  S→(S•)\n",
    "\n",
    "Finally you recieve last token `)` and use it put everything into one big `S`.\n",
    "\n",
    "**TO BE CONTINUED...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Items\n",
    "Positions within the rules are called LR items.\n",
    "In other words LR items are just rules with dot in body, e.g. `E → E •+ B`, `S → S •S`, `S → ( )•`.\n",
    "Items indicate that the parser has recognized a string correspondig to the part of rule before the dot,\n",
    "e.g. `E → E * •B` means that the parser has recognized `E` and `*` on the input and now expects to read `B`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S → S • S\n"
     ]
    }
   ],
   "source": [
    "class Item(namedtuple(\"Item\", \"rule dot_position\")):\n",
    "    def __str__(item):\n",
    "        body = list(item.rule.body)\n",
    "        body.insert(item.dot_position, \"•\")\n",
    "        return item.rule.head + \" → \" + \" \".join(body)\n",
    "\n",
    "print(Item(rule=rules[0], dot_position=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closure of items\n",
    "Closure of a set of items is the set combined with items that can be obtained\n",
    "by pushing the dot from the head into the body of a rule,\n",
    "e.g.\n",
    "    \n",
    "    closure of S → ( •S ) =\n",
    "        S → ( •S )\n",
    "        S → •S S\n",
    "        S → •( S )\n",
    "        S → •( )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   closure of S → S • S =\n",
      "\tS → • ( S )\n",
      "\tS → • S S\n",
      "\tS → S • S\n",
      "\tS → • ( )\n"
     ]
    }
   ],
   "source": [
    "def close(grammar, items):\n",
    "    closure, rules = set(items), grammar.rules\n",
    "    for rule, dot_position in items:\n",
    "        variable = (rule.body + (None,))[dot_position]\n",
    "        closure |= {Item(rule, 0) for rule in rules if rule.head == variable}\n",
    "    return close(grammar, closure) if closure > items else frozenset(closure)\n",
    "                    \n",
    "grammar = Grammar(variables, terminals, rules, start_symbol)\n",
    "item = Item(rule=rules[0], dot_position=1)\n",
    "print(f\"   closure of {item} =\\n\\t\"+\"\\n\\t\".join(map(str, close(grammar, {x}))))\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Btw, it's quite convenient to have this functionality as part of grammar class.\n",
    "So I will bind it to the class as a method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grammar.close = close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### States (sets of items)\n",
    "The core idea of LR parser is that its states are just sets of possible items.\n",
    "When parser have already read something from input, it doesn't \"know\" yet what\n",
    "rule he is going to apply and what AST node he is going to build,\n",
    "but he does know what items correspond to already read symbols.\n",
    "Actually all possible items corresponding to some state fully specify this state.\n",
    "And the set of all possible sets of items is finite.\n",
    "Thus number of states is finite.\n",
    "And we are going to precompute all the states.\n",
    "But before all that we need a class for set of items.\n",
    "\n",
    "With purpose of saving memory a set of items can be represented by items that\n",
    "can't be computed as closure of other items in this set.\n",
    "For example set {`E → E * •B`, `B → •1`, `B → •0`} can be represented by item\n",
    "`E → E * •B` alone, since items `B → •1`, `B → •0` can be obtained by finding closure of `E → E * •B`.\n",
    "So the rule `E → E * •B` is a **kernel item** of set {`E → E * •B`, `B → •1`, `B → •0`}.\n",
    "\n",
    "How to understand which items are kernel items?\\\n",
    "In general case it's quite complex and requires topological sort of rules...\n",
    "However, according to the Dragon Book, only a small subset of item sets appears during parsing,\n",
    "and all of them have kernel items with non-zero dot position.\n",
    "In other words we can just compare dot position with zero to find out if item is a kernel item.\n",
    "\n",
    "Here is the class `ItemSet` that implements memory effective representation of item sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set {S → S • S, S → • ( ), S → • ( S ), S → • S S} has kernel items S → S • S\n"
     ]
    }
   ],
   "source": [
    "class ItemSet(namedtuple(\"ItemSet\", \"grammar kernel_items\")):\n",
    "    @staticmethod\n",
    "    def from_items(grammar, items):\n",
    "        kernel_items = {item for item in items if item.dot_position > 0}\n",
    "        return ItemSet(grammar, frozenset(kernel_items))\n",
    "    \n",
    "    def __iter__(self):\n",
    "        yield from self.grammar.close(self.kernel_items)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"{\" + \", \".join(sorted(map(str, self))) + \"}\"\n",
    "\n",
    "    def __bool__(self):\n",
    "        return bool(self.kernel_items)\n",
    "################################################################################\n",
    "items = ItemSet.from_items(grammar, {item})\n",
    "print(f\"Set {items} has kernel items\", \", \".join(map(str, s.kernel_items)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GOTO\n",
    "\n",
    "    GOTO(current_parser_state, next_symbol) -> next_parser_state\n",
    " \n",
    "The GOTO function computes next parser state(item set)\n",
    "based on its current state(item set).\n",
    "Since a state is just a set of items, the function is pretty straightforward:\n",
    "assuming `next_symbol=Y` for every item `W → X •Y Z` from current set of items\n",
    "add `W → X Y• Z` into the next state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goto({B → •0, B → •1, E → E * •B}, \"1\")  =  {B → 1•}\n"
     ]
    }
   ],
   "source": [
    "def goto(grammar, items, next_symbol):\n",
    "    next_items = set()\n",
    "    for item in items:\n",
    "        if item.next_symbol == next_symbol:\n",
    "            next_item = Item(item.variable, item.body, item.dot_position + 1)\n",
    "            next_items.add(next_item)\n",
    "    return ItemSet.from_items(grammar, next_items)\n",
    "print(f'goto({s}, \"1\")  = ', goto(grammar, s, \"1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The states precomputed\n",
    "We can use the functoin `goto()` to precompute all reachable states of parser.\n",
    "With this purpose in mind we will need a starting state, a starting item.\n",
    "We need a rule that will contain a starting symbol in its body.\n",
    "So we augment our grammar with such a rule:\n",
    "we add new start symbol `START` and new rule `START → $ OLD_START $`,\n",
    "where `$` denotes start or end of input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start symbol: START\n",
      "variables: B, E, START\n",
      "terminals: '$', '*', '+', '0', '1'\n",
      "rules:\tB → 0\n",
      "\tB → 1\n",
      "\tE → B\n",
      "\tE → E * B\n",
      "\tE → E + B\n",
      "\tSTART → $ E $\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def augment_grammar(grammar):\n",
    "    variables, rules, start = grammar.variables, grammar.rules, grammar.start\n",
    "    new_start = \"START\"\n",
    "    # if variable START is already in the grammar, we use START' or START'' ...\n",
    "    while new_start in variables:\n",
    "        new_start += \"'\"\n",
    "    new_variables = variables | {new_start}\n",
    "    new_terminals = grammar.terminals | {\"$\"}\n",
    "    new_rules = [(new_start, (\"$\", start, \"$\"))] + rules\n",
    "    return Grammar(new_variables, new_terminals, new_rules, new_start)\n",
    "augmented_grammar = augment_grammar(grammar)\n",
    "print(augmented_grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can precompute all the states that are reachable from item `START → $ •OLD_START $`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: {B → •0, B → •1, E → •B, E → •E * B, E → •E + B, START → $ •E $}\n",
      "1: {B → 0•}\n",
      "2: {B → 1•}\n",
      "3: {E → B•}\n",
      "4: {E → E •* B, E → E •+ B, START → $ E •$}\n",
      "5: {START → $ E $•}\n",
      "6: {B → •0, B → •1, E → E * •B}\n",
      "7: {B → •0, B → •1, E → E + •B}\n",
      "8: {E → E * B•}\n",
      "9: {E → E + B•}\n"
     ]
    }
   ],
   "source": [
    "def grammar_states(grammar):\n",
    "    assert grammar.rules[0][0] == grammar.start\n",
    "    variables, rules, start = grammar.variables, grammar.rules, grammar.start\n",
    "    symbols = sorted(grammar.variables | grammar.terminals)\n",
    "    start_state = ItemSet.from_items(grammar, {Item(*rules[0], 1)})\n",
    "    states = [start_state]\n",
    "    states_lookup = {start_state}\n",
    "    processed_states = 0\n",
    "    while processed_states < len(states):\n",
    "        state = states[processed_states]\n",
    "        processed_states += 1\n",
    "        for symbol in symbols:\n",
    "            new_state = goto(grammar, state, symbol)\n",
    "            if new_state and new_state not in states_lookup:\n",
    "                states_lookup.add(new_state)\n",
    "                states.append(new_state)\n",
    "    return states\n",
    "\n",
    "states = grammar_states(augmented_grammar)\n",
    "for i, state in enumerate(states):\n",
    "    print(f\"{i}: {state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions precomputed\n",
    "We have states precomputed. Cool! Now let's precompute actions that should be executed.\n",
    "For each possible state and each posible terminal on input we will compute desired action.\n",
    "\n",
    "LR parser supports these types of actions:\n",
    "1. SHIFT: push the terminal from input into the stack.\n",
    "2. REDUCE: pack a few symbols from stack into an AST node.\n",
    "3. ACCEPT: accept current stack as succesfully built AST tree. \n",
    "4. DIE: raise an exception if there is no reasanable action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 '$' -> die START \t {B → •0, B → •1, E → •B, E → •E * B, E → •E + B, START → $ •E $}\n",
      "0 '*' -> die START \t {B → •0, B → •1, E → •B, E → •E * B, E → •E + B, START → $ •E $}\n",
      "0 '+' -> die START \t {B → •0, B → •1, E → •B, E → •E * B, E → •E + B, START → $ •E $}\n",
      "0 '0' -> shift 1 \t {B → •0, B → •1, E → •B, E → •E * B, E → •E + B, START → $ •E $}\n",
      "0 '1' -> shift 2 \t {B → •0, B → •1, E → •B, E → •E * B, E → •E + B, START → $ •E $}\n",
      "1 '$' -> reduce B 1 \t {B → 0•}\n",
      "1 '*' -> reduce B 1 \t {B → 0•}\n",
      "1 '+' -> reduce B 1 \t {B → 0•}\n",
      "1 '0' -> reduce B 1 \t {B → 0•}\n",
      "1 '1' -> reduce B 1 \t {B → 0•}\n",
      "2 '$' -> reduce B 1 \t {B → 1•}\n",
      "2 '*' -> reduce B 1 \t {B → 1•}\n",
      "2 '+' -> reduce B 1 \t {B → 1•}\n",
      "2 '0' -> reduce B 1 \t {B → 1•}\n",
      "2 '1' -> reduce B 1 \t {B → 1•}\n",
      "3 '$' -> reduce E 1 \t {E → B•}\n",
      "3 '*' -> reduce E 1 \t {E → B•}\n",
      "3 '+' -> reduce E 1 \t {E → B•}\n",
      "3 '0' -> reduce E 1 \t {E → B•}\n",
      "3 '1' -> reduce E 1 \t {E → B•}\n",
      "4 '$' -> accept E \t {E → E •* B, E → E •+ B, START → $ E •$}\n",
      "4 '*' -> shift 6 \t {E → E •* B, E → E •+ B, START → $ E •$}\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "def precompute_actions(grammar):\n",
    "    states, terminals = grammar_states(grammar), sorted(grammar.terminals)\n",
    "    actions = {}\n",
    "    for i, state in enumerate(states):\n",
    "        kernel_item = next(iter(state.kernel_items))\n",
    "        variable, body = kernel_item.variable, kernel_item.body\n",
    "        for next_terminal in sorted(grammar.terminals):\n",
    "            actions[i, next_terminal] = (\"die\", variable)\n",
    "        for next_terminal in sorted(grammar.terminals - {'$'}):\n",
    "            next_state = goto(grammar, state, next_terminal)\n",
    "            if next_state:\n",
    "                actions[i, next_terminal] = (\"shift\", states.index(next_state))\n",
    "        if any(i.variable == grammar.start and i.dot_position == 2 for i in state.kernel_items):\n",
    "            actions[i, \"$\"] = (\"accept\", variable)\n",
    "        if kernel_item.dot_position == len(kernel_item.body):\n",
    "            for next_terminal in sorted(grammar.terminals):\n",
    "                actions[i, next_terminal] = (\"reduce\", variable, len(body))\n",
    "    return actions\n",
    "\n",
    "actions = precompute_actions(augmented_grammar)\n",
    "for situation, action in list(actions.items())[:22]:\n",
    "    state_index, terminal = situation\n",
    "    print(state_index, repr(terminal), \"->\", *action, \"\\t\", states[state_index])\n",
    "if len(actions) > 22: print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- '$' '*' '+' '0' '1'\n",
      " 0 dST dST dST s1  s2  \t {B → •0, B → •1, E → •B, E → •E * B, E → •E + B, START → $ •E $}\n",
      " 1 rB  rB  rB  rB  rB  \t {B → 0•}\n",
      " 2 rB  rB  rB  rB  rB  \t {B → 1•}\n",
      " 3 rE  rE  rE  rE  rE  \t {E → B•}\n",
      " 4 aE  s6  s7  dE  dE  \t {E → E •* B, E → E •+ B, START → $ E •$}\n",
      " 5 rST rST rST rST rST \t {START → $ E $•}\n",
      " 6 dE  dE  dE  s1  s2  \t {B → •0, B → •1, E → E * •B}\n",
      " 7 dE  dE  dE  s1  s2  \t {B → •0, B → •1, E → E + •B}\n",
      " 8 rE  rE  rE  rE  rE  \t {E → E * B•}\n",
      " 9 rE  rE  rE  rE  rE  \t {E → E + B•}\n"
     ]
    }
   ],
   "source": [
    "table = [[\"   \"] * (len(augmented_grammar.terminals)) for i in range(len(states))]\n",
    "terminals = sorted(augmented_grammar.terminals)\n",
    "for situation, action in list(actions.items()):\n",
    "    state_index, terminal = situation\n",
    "    table[state_index][terminals.index(terminal)] = f\"{action[0][0]}{str(action[1])[:2]:2}\"\n",
    "print(\"--\", \" \".join(map(repr, terminals)))\n",
    "for i, row in enumerate(table):\n",
    "    print(f\"{i:2}\", \" \".join(row), \"\\t\", states[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gotos precomputed\n",
    "We precomputed the actions, why not precompute goto(...) results? We need them for nonterminals to know\n",
    "what state to go when reducing something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 'B'): 3, (0, 'E'): 4, (6, 'B'): 8, (7, 'B'): 9}\n"
     ]
    }
   ],
   "source": [
    "def precompute_gotos(grammar):\n",
    "    states = grammar_states(grammar)\n",
    "    gotos = {}\n",
    "    for i, state in enumerate(states):\n",
    "        for variable in sorted(grammar.variables):\n",
    "            next_state = goto(grammar, state, variable)\n",
    "            if next_state:\n",
    "                gotos[i, variable] = states.index(next_state)\n",
    "    return gotos\n",
    "gotos = precompute_gotos(augmented_grammar)\n",
    "print(gotos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime of the parser\n",
    "Using all the precomputed information we can now write the parser, that uses only numbers/indexes of states, not the states itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'E'\n",
      " ├'B'\n",
      " │ ├'E'\n",
      " │ │ ├'B'\n",
      " │ │ │ ├'1'\n",
      " │ ├'+'\n",
      " │ ├'1'\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def parser(actions, gotos, DEBUG=False):\n",
    "    def parse(source):\n",
    "        stack = [(\"$\", 0)]\n",
    "        i, tokens = 0, list(source) + [\"$\"]\n",
    "        while True:\n",
    "            token = tokens[i]\n",
    "            last_token, state = stack[-1]\n",
    "            action = actions[state, token] \n",
    "            if DEBUG: print(stack, \"<<<\", repr(token), \":\", action)\n",
    "            if action[0] == \"shift\":\n",
    "                next_state, i = action[1], i + 1\n",
    "                stack.append(([token], next_state))\n",
    "            elif action[0] == \"reduce\":\n",
    "                variable, size = action[1:]\n",
    "                stack, node = stack[:-size], stack[size:]\n",
    "                next_state = gotos[stack[-1][1], variable]\n",
    "                stack.append(([variable] + [n for n, _ in node], next_state))\n",
    "                #stack.append((variable, next_state))\n",
    "            elif action[0] == \"accept\":\n",
    "                return stack[1][0]\n",
    "            else:\n",
    "                raise Exception(f\"{repr(stack)} <<< {token}\")\n",
    "    return parse\n",
    "\n",
    "def print_ast(ast, offset = 0):\n",
    "    head, children = ast[0], ast[1:]\n",
    "    print(\" │\" * (offset - 1) + \" ├\" * bool(offset) + repr(head))\n",
    "    for child in children:\n",
    "        print_ast(child, offset + 1)\n",
    "\n",
    "parse = parser(actions, gotos)\n",
    "print_ast(parse(\"1+1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'E'\n",
      " ├'B'\n",
      " │ ├'E'\n",
      " │ │ ├'B'\n",
      " │ │ │ ├'E'\n",
      " │ │ │ │ ├'B'\n",
      " │ │ │ │ │ ├'1'\n",
      " │ │ │ ├'*'\n",
      " │ │ │ ├'0'\n",
      " │ ├'+'\n",
      " │ ├'1'\n"
     ]
    }
   ],
   "source": [
    "print_ast(parse(\"1*0+1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'E'\n",
      " ├'B'\n",
      " │ ├'1'\n"
     ]
    }
   ],
   "source": [
    "print_ast(parse(\"1\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
